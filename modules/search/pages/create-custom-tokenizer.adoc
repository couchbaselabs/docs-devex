= Create a Custom Tokenizer 
:page-topic-type: guide
:page-ui-name: {ui-name}
:page-product-name: {product-name}
:description: Create a custom tokenizer with the Couchbase {page-ui-name} to change how the Search Service creates tokens for matching Search index content to a Search query.
:page-toclevels: 3

[abstract]
{description}

== Prerequisites 

* You have the Search Service enabled on a node in your cluster.
For more information about how to change Services on your cluster, see xref:cloud:clusters:modify-database.adoc[].

 
* You have logged in to the Couchbase {page-ui-name}. 

* You have started to create or already created an index in xref:create-search-index-ui.adoc[Advanced Mode].

== Procedure

You can create 2 types of custom tokenizers: 

|====
|Tokenizer Type |Description

|<<regexp,Regular expression>> |The tokenizer uses any input that matches the regular expression to create new tokens. 

|<<excep,Exception>> |The tokenizer removes any input that matches the regular expression, and creates tokens from the remaining input. You can choose another tokenizer to apply to the remaining input.

|====

[#regexp]
=== Create a Regular Expression Tokenizer

To create a regular expression tokenizer with the {page-ui-name}:

. On the *Operational Clusters* page, select the cluster that has the Search index you want to edit. 
. Go to menu:Data Tools[Search].
. Click the index where you want to create a custom tokenizer.
. Under *Advanced Settings*, expand *Custom Filters*. 
+
NOTE: Make sure you use *Advanced Mode*.  
. Click btn:[Add Tokenizer].
. In the *Name* field, enter a name for the custom tokenizer. 
. In the *Type* list, select *regexp*.
. In the *Regular Expression* field, enter the regular expression to use to split input into tokens. 
. Click btn:[Submit].

[#excep]
=== Create an Exception Custom Tokenizer 

To create an exception custom tokenizer with the {page-ui-name} in Advanced Mode:

. On the *Operational Clusters* page, select the cluster that has the Search index you want to edit. 
. Go to menu:Data Tools[Search].
. Click the index where you want to create a custom tokenizer.
. Expand *Custom Filters*. 
. Click btn:[Add Tokenizer].
. In the *Name* field, enter a name for the custom tokenizer. 
. In the *Type* list, select *exception*.
. In the *New Word* field, enter a regular expression to use to remove content from input.
. To add the regular expression to the list of exception patterns, click btn:[Add].
. (Optional) To add additional regular expressions to the list of exception patterns, repeat the previous steps.
. In the *Tokenizer for Remaining Input* list, select a tokenizer to apply to input after removing any content that matches the regular expression.
+
For more information about the available tokenizers, see xref:default-tokenizers-reference.adoc[].
. Click btn:[Submit].

== Next Steps

After you create a custom tokenizer, you can use it with xref:create-custom-analyzer.adoc[a custom analyzer].

To continue customizing your Search index, you can also: 

* xref:set-advanced-settings.adoc[]
* xref:set-type-identifier.adoc[]
* xref:create-type-mapping.adoc[]
* xref:create-child-field.adoc[]
* xref:create-child-mapping.adoc[]
* xref:create-custom-character-filter.adoc[]
* xref:create-custom-token-filter.adoc[]
* xref:create-custom-wordlist.adoc[]

To run a search and test the contents of your Search index, see xref:simple-search-ui.adoc[].