= Use Vector Indexes for AI Applications
:page-topic-type: concept
:page-ui-name: {ui-name}
:page-product-name: {product-name}
:description: Use Couchbase {page-product-name}'s Vector Index features to find documents based on content similarity or semantic meaning.

{description}

Vector Indexing lets you add vectors to Couchbase {product-name} indexes. 
You store index values as attributes in your documents, and then index those attributes.
You can then find relevant data by executing queries on the vector attributes, which use the index to find similar vectors.

== About Vectors

Vectors are a numerical representation of complex data such as text, images, or audio.
They distill this complex data into an array of floating-point values called dimensions. 
These dimensions capture features of the data in a way that makes the data easy to compare. 
Similar data, such as text about the same topic or images of a similar subject, have similar vectors. 
These vectors are close together the multi-dimensional space formed by all of the vectors in the dataset.
Vectors for dissimilar data (for example, articles discussing two different topics) have vectors that are further apart in this multi-dimensional space.

By comparing two vectors, you can find the similarity between two pieces of data.
This similarity goes beyond just finding parts of the data that's identical (for example, finding many of the same words in two pieces of text).
The vectors represent features such as the semantic meaning of text rather than just superficial similarities.

In Couchbase {product-name}, you can use vectors to find documents that are similar to an existing document or query values.
By searching your database for similar vectors, you can identify data that's semantically similar to your query value.

=== Embedding Vectors

Embedding is the process of generating vectors that represent a piece of complex data.
You use a model that's specific to the data's type to generate its vector.
For example, to generate vectors from text, you can use a use models such as https://en.wikipedia.org/wiki/Word2vec[Word2Vec^] or https://en.wikipedia.org/wiki/GloVe[Global Vectors for Word Representation (GloVe)^].
The models take into account the context around each potion of the text when generating a vector.
This context is what captures the semantic meaning of the text and embeds it in the vector's dimensions.

You can only compare the similarity of vectors when the same model generated them. 
For example, you cannot find the similarity of two pieces of text if you generated one text's vector using Word2Vec and you used GloVe for the other.

Couchbase {product-name} does not implement any embedding models. 
You must use external embedding models to generate them.
A common method of embedding vectors in your database is to call the https://openai.com/index/introducing-text-and-code-embeddings/[OpenAI Embedding API^] to generate vectors for your data.   
You then store the vector as an attribute in your database.

=== Vector Similarity

Once you have embedded complex data as a vector in your database, you find data with a similar meaning by locating similar vectors. 
Several algorithms exist that you can use to find similar vectors. 
They're also called distance functions, as they determine the distance between vectors in the vector space. 
These algorithms are broken into two categories:

* *K-Nearest Neighbor (KNN)* methods use precise algorithms to determine the distance between two vectors.  
This precision comes at a higher computational cost. 
They are best used with smaller vector datasets and when precision is more important than computational resource use.

* *Approximate Nearest Neighbor (ANN)* uses methods that approximate the distance between two vectors. 
They use less computational power, but at the cost of less precise results.
These methods are not guaranteed to return the closest matches to a particular vector. 
Use one of these methods when limiting computing resource use is more important than precise results, especially with a large dataset.

[#quantize]
=== Reducing Vector Complexity

The number of dimensions in a vector can vary. 
A vector that represents highly complex data such as images can contain several thousand dimensions.
Less complex  data, such as text, can still have hundreds to over a thousand dimensions.
When you execute a query to find similar vectors, this large number of dimensions can make the query computationally expensive.
A search for similar raw vectors in an index requires comparing hundreds or thousands of values for each index entry.

Several techniques (collectively known as quantization) can reduce the size of vectors, making them faster to process.
This reduction in space comes at the cost of lower precision. 

A technique called Product Quantization (PQ) reduces the size of the vectors by finding representative sub-vectors (portions of the full vector), called centroids, in the dataset's vector space. 
It saves the centroids in a list called a codebook.
It then breaks each vector in the dataset into sub-vectors and maps these portions to the closest centroid in the codebook. 
Instead of storing the full original vector, this technique stores just the indexes of these centroids in the codebook instead of the full vector value.
It reduces the storage size without altering the dimensionality of the vector.

Another technique, Scalar Quantization reduces the size of a vector by mapping its floating point dimensions to a smaller data type, such as an integer. 
In doing so, it does lose accuracy. 
However, it makes the vector computationally easier to process.
And the process of quantization is faster than PQ, because it does not need to build a codebook.

Some types of Couchbase {product-name} indexes perform quantization on your vectors to improve performance and reduce storage requirements.  

== Vectors in Couchbase {product-name} Indexes

To use vectors in Couchbase {product-name} queries and searches, you store them in attributes in your documents and then add those attributes to an index. 
Couchbase {product-name} supports vectors in the following index types:

* Vector Indexes are specifically designed for vector searches. 
They can index billions of documents.
Most of the Vector Index resides in a highly optimized format on disk.
Use this type of index when you want to primarily query vector values with a low memory footprint.
They excel at traditional vector similarity and semantic searches.
This index offers high accuracy even for vectors with a large number of dimensions. 

* Global Secondary Indexes (GSI) can be extended with a single vector column. 
Adding a vector to a GSI is useful when your queries combine a single vector value with standard scalar attributes.
The scalar attributes in a query reduce the number of vectors the Couchbase {product-name} has to scan to find similar vectors.
Use GSI indexes with a vector column when you want to perform hybrid searches of documents using both scalars and a vector.
They work well for cases where your queries are highly selective---returning a small number of results from a large dataset.
They consume a moderate amount of memory and can index billions of documents.

* A Search index used for Full-Text Search can have an added vector attribute,
The search results combine keyword matching of Full-Text Search with the semantic meaning matching of vectors to give you hybrid results.
Use Search indexes with a vector attribute when you want to query based on keywords, geospacial data, and a vector value.
These indexes can index millions of documents.


== Uses for Vectors in Couchbase {product-name} Indexes

Here's some ways you can use vector indexes in your applications:

* Improve your application's search ability by querying not just for discrete values, but also based on similar content.
For example, suppose your application is a product catalog. 
Using a GSI index extended with a vector column, it could find similar products based not just on attributes (such as color or size) but also the similarity of the product's description and customer reviews.

* Create a customized chatbot based on your own data by implementing a  Retrieval-Augmented Generation (RAG) workflow using a Vector Index.
When a user enters a question, convert it to a vector using the same embedding model you used to embed vectors into your database.
Perform a query using a Vector Index to find relevant documents in your database.
Then pass the user's question and the relevant documents to a Large Language Model (LLM) to generate a final response for the user. 

== Application Workflow with Vector Indexes

No matter which type of Couchbase {product-name} index you use, the workflow for your application to use them is similar, as shown in the following diagram.

[plantuml,ai-app-workflow,svg]
....
include::vector-index:partial$ai-app-workflow.puml[]
....

. Your application often needs to load or update data that contains one or more attributes that have embedded vectors in Couchbase {product-name}. 
Before it can load the data, your application calls an embedding model to create vectors for these attributes.

. The application then sends the data including the embedded vector to Couchbase {product-name} for storage.
The Data Service handles creating or updating a document in a bucket.

. The addition or update of data in an indexed collection causes the Index Service to update the index. 

. When your application needs to perform a query or search that includes a vector, it uses the  embedding model to generate a vector for the search value. 
It then passes this vector as part of the search request or query.

. Depending if the application is performing a search or a query, the Search Service or Query Service processes the request using the appropriate index

. The Search or Query Service returns any results to your application.

